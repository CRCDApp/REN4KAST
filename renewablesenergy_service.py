# -*- coding: utf-8 -*-
"""RenewablesEnergy_Service.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bE50o9uxZReZo-IhCJ8Ms20mbYHZMvK9

# 1.   Gathering Data and Save into a CSV File

#connecting to drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Get request method"""

import json
import requests
from datetime import datetime, timedelta



cities=[      "memmingen" ,"Rostock Warnem-u00fcnde","Osnabrueck","Braunschweig","Cuxhaven","Luebeck","Berlin","Bonn","Hof","Freudenstadt","MÃ¼nchen","Meiningen"]
station_Ids = ["10947",   "10170" ,    "10315"    ,"10348"  ,"10131"  ,"10156"  ,"10382"  ,"10513"  ,"10685"  ,"10815"  ,"10865"  ,"10548"]
latitude=     ["47.9833","54.1833"    ,"52.1333"  ,"52.3"   ,"53.8667","53.8167","52.5667","50.8667","50.3167","48.45"  ,"48.1333","50.5667"]
longitude=    ["10.2333","12.0833"    ,"7.7"      ,"10.45"  ,"8.7"    ,"10.7"   ,"13.3167","7.1667" ,"11.8833","8.4167" ,"11.55"  ,"10.3833"]
altitude=     ["634",     "4" ,       "48"        ,"81"     ,"5"      ,"14"     ,"37"     ,"91"     ,"565"    ,"797"    ,"520"    ,"450"]


start=datetime.today().strftime('%Y-%m-%d')
end=(datetime.today()+timedelta(days=1)).strftime('%Y-%m-%d')
altitude = -999

def get_request(label="", latitude=44.083, longitude=5.059, altitude=-999,start='2017-01-01',end='2017-01-02'):
  url = 'http://www.soda-pro.com/api/jsonws/helioclim3-forecast-portlet.hc3request/proxy?url=http%253A%252F%252Fwww.soda-is.com%252Fcom%252Fhc3v5_meteo_soda_get.php%253Flatlon%253D{}%252C{}%2526alt%253D-999%2526date1%253D{}%2526date2%253D{}%2526summar%253D15%2526refTime%253DUT%2526tilt%253D0%2526azim%253D180%2526al%253D0.2%2526horizon%253D1%2526outcsv%253D1%2526forecast%253D2%2526gamma-sun-min%253D12%2526header%253D1%2526code%253D1%2526format%253Dunified'.format(latitude,longitude,start,end)
  resp = requests.get(url).content
  link = str(resp).split("value>")
  
  csvfile=requests.get(link[1][:len(link[1])-2]).content
  
  csvfilestring = csvfile.decode("utf-8")
  #removing headers
  prepared_data = csvfilestring.splitlines()[31:]
  # saving in to a csv file
  with open('/content/drive/My Drive/Colab Notebooks/Renewables/RadiationData/{}.csv'.format(label), 'w') as filehandle:
    for listitem in prepared_data:
        listitem = listitem.replace(';',',')
        filehandle.write('%s\n' % listitem)

"""#Defining columns and cities, Getting station IDs which needs to be set manually"""

for i in range(len(cities)):
  get_request(label="realtime{}-{}___{}".format(cities[i],start,end), latitude=latitude[i], longitude=longitude[i], altitude=altitude,start=start,end=end)
  print(cities[i])

"""#removing extra columns & updating wind speed

m/s to km/h ~ 3.6
"""

import pandas as pd

def write_into_csv(output=[],name="merged",columns=[]):
  output.to_csv('/content/drive/My Drive/Colab Notebooks/Renewables/RadiationData/{}.csv'.format(name), columns=columns)

def cleaning_initial_data(cities=[], start="2017-01-01", end="2017-01-01"):
    df = []
    for i in range(len(cities)):
      df.append(pd.read_csv('/content/drive/My Drive/Colab Notebooks/Renewables/RadiationData/{}.csv'.format("realtime{}-{}___{}".format(cities[i],start,end)), sep=','))
      df[i].index = pd.date_range(start="{} 00:00:00".format(start), periods=len(df[i]), freq='15Min')
      df[i].index.name = 'time'
      df[i]['Wind speed'] = [element * 3.6 for element in df[i]['Wind speed']]
      # dropping old time column
      #df[i].drop(df[i].columns[0], axis=1)
      print("\n File name: {}-{}___{} \n\n".format(cities[i],start,end))
      write_into_csv(output=df[i],name="new{}-{}___{}".format(cities[i],start,end),columns=['Global Horiz','Wind speed'])
    return df
df = cleaning_initial_data(cities=cities, start=start, end=end)

import pandas as pd

import pandas as pd

def read_data(cities=[], start="2017-01-01", end="2017-01-01"):
    df = []
    for i in range(len(cities)):
      df.append(pd.read_csv('/content/drive/My Drive/Colab Notebooks/Renewables/RadiationData/{}.csv'.format("new{}-{}___{}".format(cities[i],start,end)), sep=','))
      df[i].index = pd.date_range(start="{} 00:00:00".format(start), periods=len(df[i]), freq='15Min')
      df[i].index.name = 'time'
      # dropping time column
      #df[i] = df[i].drop(columns=['time'])
      print(" File name: {}-{}___{} \n".format(cities[i],start,end))
    return df
df = read_data(cities=cities, start=start, end=end)

def show_data_stats(input_data):
  mean = input_data.mean()
  variance = input_data.var()
  standardDeviation = input_data.std()
  min = input_data.min()
  max = input_data.max()
  dataStatistics = pd.concat([mean, variance, standardDeviation, min, max], axis=1)
  dataStatistics.columns = ['Mean', 'Variance', 'Standard Deviation', 'Min', 'Max']
  return dataStatistics

show_data_stats(df[0])

"""#2.   Preparing Data For getting merged"""

import datetime
#   *****   Important Assumption    *****
# taget_columns[0] is time stamp
def merge_datasets_by_taking_average(df=[], target_columns=[], cities=[]):
    output=[]
    for row in range(len(df[0])):
        output.append([])
        for col in target_columns:
          sum=0
          for i in range(len(cities)):
              
              if (col==target_columns[0]):
                if (df[i][target_columns[0]][row] != df[0][target_columns[0]][row]):
                  print("ERROR: TIME did not match!", row, cities[i])
                  return 
              else:
                sum = sum + float(df[i][col][row])
          if(col==target_columns[0]):
            output[row].append(df[0][target_columns[0]][row])  
          else:
            mean = sum/(len(cities))
            output[row].append(round(mean,2))
            
        print(output[row])
    return output

import csv
def write_merged_into_csv(output=[],name="merged",columns=[]):
  with open('/content/drive/My Drive/Colab Notebooks/Renewables/RadiationData/{}.csv'.format(name), 'w', newline='') as csvfile:
          csv_writer = csv.writer(csvfile, delimiter=',',
                                  quotechar='|', quoting=csv.QUOTE_MINIMAL)
          csv_writer.writerow(columns)
          for xx in output:
              csv_writer.writerow([xx[i] for i in range(len(columns))])

dataframe = read_data(cities=cities, start=start, end=end)

#dropping first column
target_columns = dataframe[0].columns

output = merge_datasets_by_taking_average(df=dataframe, target_columns=target_columns, cities=cities)

write_merged_into_csv(output=output,name="RadiationDatazaq",columns=target_columns)

dataframe[0].index[10]

def show_data_stats(input_data):
  mean = input_data.mean()
  variance = input_data.var()
  standardDeviation = input_data.std()
  min = input_data.min()
  max = input_data.max()
  dataStatistics = pd.concat([mean, variance, standardDeviation, min, max], axis=1)
  dataStatistics.columns = ['Mean', 'Variance', 'Standard Deviation', 'Min', 'Max']
  return dataStatistics


df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Renewables/RadiationData/RadiationDatazaq.csv', sep=',',index_col=0)
show_data_stats(df)

df

"""#GETTING ENTSOE REAL TIME DATA"""

!python3 -m pip install entsoe-py

from entsoe import EntsoePandasClient
import pandas as pd
from entsoe import EntsoeRawClient

client = EntsoePandasClient("94aa148a-330b-4eee-ba0c-8a5eb0b17825")

start = pd.Timestamp('20200801', tz='Etc/GMT')
end = pd.Timestamp('20200803', tz='Etc/GMT')
country_code = 'DE'  # Germany

# methods that return Pandas Series
# dar in method 
#'documentType': 'A75',
#'processType': 'A16',
#https://github.com/EnergieID/entsoe-py/blob/5d176699472744c1acef546410826da6549112cf/entsoe/entsoe.py#L270
#
entsoe_data = client.query_generation(country_code, start=start,end=end, psr_type=None)

entsoe_data.to_csv('/content/drive/My Drive/Colab Notebooks/Renewables/ENTSOE-DATA/ENTSOE-{}-{}-{}.csv'.format(country_code,start,end), sep=',', encoding='utf-8')
entsoe_data.head()
#converting to GMT
entsoe_data.index = entsoe_data.index.tz_convert('Etc/GMT')
entsoe_data.to_csv('/content/drive/My Drive/Colab Notebooks/Renewables/ENTSOE-DATA/GMT-ENTSOE-{}-{}-{}.csv'.format(country_code,start,end), sep=',', encoding='utf-8')

data_normal = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Renewables/ENTSOE-DATA/ENTSOE-{}-{}-{}.csv'.format(country_code,start,end), index_col=0)
data_normal.head()
data_gmt = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Renewables/ENTSOE-DATA/GMT-ENTSOE-{}-{}-{}.csv'.format(country_code,start,end), index_col=0)
data_gmt.tail()

def search_and_print_missing_days(search=[],column=[]):
  for s in search:
    count = 0
    for ii in column:
      if (s in ii):
       count = count+1
    # Here we are using 24 because we have hourly data
    # !=0 is because of the months that have less than 31 days. 
    #This could be adjusted in the input is precise about the days of the months
    #if (count<24 and count != 0):
    if (count != 96):
        print(s,"\t", count)

def test_show_null_columns_from_target_columns(data):
  # the following line could generate null values by the defined method
  #df[i] = df[i].interpolate(method='linear')
  for col in data.columns:
        if (len(data[data[col].isnull()]) > 0):
          print(col,"\t", len(data[data[col].isnull()]))
          # Could also print the null indexes
          print(data[data[col].isnull()].index.tolist())

def fix_and_check_null_values_in_columns(columnName, inputData):
  newData = inputData.copy()
  newData[columnName] = newData[columnName].interpolate(method='linear')
  #newData[columnName] = newData[columnName].interpolate(method='spline', order=2)
  for index in range(len(inputData)):
    if(inputData[columnName][index] != newData[columnName][index]):
       print('INDEX: \t {} \t Old: \t {} \t New: \t {} \t Before: \t {} \t After: \t {}'.format(newData.index[index],inputData[columnName][index],newData[columnName][index],inputData[columnName][index-1],inputData[columnName][index+1]))
  return newData
  #fix_and_check_null_values_in_columns('Wind Onshore', data_gmt)

def show_data_stats(input_data):
  mean = input_data.mean()
  variance = input_data.var()
  standardDeviation = input_data.std()
  min = input_data.min()
  max = input_data.max()
  dataStatistics = pd.concat([mean, variance, standardDeviation, min, max], axis=1)
  dataStatistics.columns = ['Mean', 'Variance', 'Standard Deviation', 'Min', 'Max']
  return dataStatistics

import calendar
year = [ 2020]
test = []
for year in year:
  for month in [8]:
    for day in [1,2,3]:
      if (month < 9 and day < 9):
        test.append(str(year)+"-0"+str(month+1)+"-0"+str(day+1))
      elif (day < 9):
        test.append(str(year)+"-"+str(month+1)+"-0"+str(day+1))
      elif (month < 9):
        test.append(str(year)+"-0"+str(month+1)+"-"+str(day+1))
      else:
        test.append(str(year)+"-"+str(month+1)+"-"+str(day+1))

search_and_print_missing_days(test,list(data_gmt.index))

test_show_null_columns_from_target_columns(data_gmt)

"""trying to remove null values but since it has 3 hours delay, error happens cause few last rows are null and it cannot handle it.

For now this step could be ignored
"""

fixedgmtdata = data_gmt.copy()
for clm in ['Biomass','Fossil Brown coal/Lignite']:
  fixedgmtdata= fix_and_check_null_values_in_columns(clm, fixedgmtdata)

fixedgmtdata.to_csv('/content/drive/My Drive/Colab Notebooks/Renewables/ENTSOE-DATA/fixednulls-GMT-ENTSOE-{}-{}-{}.csv'.format(country_code,start,end), sep=',', encoding='utf-8')
generation_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Renewables/ENTSOE-DATA/fixednulls-GMT-ENTSOE-{}-{}-{}.csv'.format(country_code,start,end), index_col=0)
generation_data.head()

sumBioMassAndHydro = generation_data['Biomass'] + generation_data['Hydro Run-of-river and poundage'] + generation_data['Hydro Water Reservoir'] + generation_data['Geothermal'] + generation_data['Waste']
sumBioMassAndHydro

sumOthers = generation_data['Wind Offshore'] + generation_data['Wind Onshore'] + generation_data['Solar'] + generation_data['Nuclear'] + generation_data['Fossil Brown coal/Lignite'] + generation_data['Fossil Hard coal'] + generation_data['Fossil Gas'] + generation_data['Hydro Pumped Storage'] + generation_data['Other'] + generation_data['Other renewable'] + generation_data['Fossil Oil'] 
print(sumOthers)

calcTotal = sumBioMassAndHydro + sumOthers
calcTotal

RenForecast = generation_data.drop(columns=['Biomass', 'Fossil Brown coal/Lignite', 'Fossil Gas',
       'Fossil Hard coal', 'Fossil Oil', 'Geothermal', 'Hydro Pumped Storage',
       'Hydro Run-of-river and poundage', 'Hydro Water Reservoir', 'Nuclear',
       'Other', 'Waste'])
RenForecast.insert(0, "calcTotal", calcTotal, True) 
RenForecast.insert(1, "sumBioMassAndHydro", sumBioMassAndHydro, True) 

RenForecast

start_index = [i for i, j in enumerate(sumBioMassAndHydro.index) if '2020-08-01 00:00:00+00:00' in j][0]
end_index = [i for i, j in enumerate(sumBioMassAndHydro.index) if '2020-08-02 00:00:00+00:00' in j][0]
print(sumBioMassAndHydro[start_index:end_index])

bioMassAndHydroAveragePowerGeneration = round(sumBioMassAndHydro[start_index:end_index].mean(),2)
bioMassAndHydroAveragePowerGeneration

renewablesPercentage = pd.DataFrame(columns=['percentage'], index=[RenForecast.index])
forecast = RenForecast.copy()
for index, row in forecast.iterrows():
    if (row['sumBioMassAndHydro']==0):
        row['sumBioMassAndHydro'] = bioMassAndHydroAveragePowerGeneration
    sum_renewables = row['sumBioMassAndHydro'] + row['Other renewable'] + row['Solar'] + row['Wind Offshore'] + row['Wind Onshore']
    prct = sum_renewables / row['calcTotal']
    prct = prct * 100
    renewablesPercentage.loc[index] = [round(prct,2)]

renewablesPercentage

renewablesPercentage.to_csv('/content/drive/My Drive/Colab Notebooks/Renewables/ENTSOE-DATA/final-GMT-ENTSOE-{}-{}-{}.csv'.format(country_code,start,end), sep=',', encoding='utf-8')

"""# TRYING DIFFERENT EXOGENOUS PARAMETERS"""

import tensorflow as tf
import multiprocessing as mp
from datetime import timedelta
print(tf.test.gpu_device_name())
print("cpu count", mp.cpu_count())
PATH = "/content/drive/My Drive/"  # @param {type:"string"}
data_path="/content/drive/My Drive/walk-forward-results/MODELSFORPLOT/"# @param {type:"string"}

from datetime import datetime, timedelta
import sys
import numpy as np, pandas as pd
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from sklearn import metrics
from math import sqrt
import matplotlib.pyplot as plt
import dateutil.parser
import ast
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import adfuller  # to do ADF test
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.arima_model import ARIMA  # to do predictions
from sklearn.model_selection import train_test_split
# from sklearn.utils import check_arrays

import requests, io
import json
import os
from pandas import read_csv
import pandas as pd
from datetime import datetime
from matplotlib import pyplot
from matplotlib import pyplot as PLT
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX  # ,SARIMAXParams,SARIMAXSpecification,SARIMAXResults
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn import metrics
from google.colab import drive

drive.mount('/content/drive')


def read_and_split_data(target_period_days=3, start_date="2019-07-01 01:00:00", days_to_test=1,
                        exog_columns=['temperature', 'dewpoint', 'humidity', 'windspeed', 'winddirection', 'pressure'],
                        file_name='twoWeeks'):
    data = pd.read_csv(PATH + '{}.csv'.format(file_name), index_col=0)
    data.index = pd.date_range(start=data.index[0], periods=len(data), freq='15Min')
    #rengeneration = pd.read_csv('/content/drive/My Drive/generationOnly.csv', index_col=0)
    #rengeneration.index = pd.date_range(start=data.index[0], periods=len(rengeneration), freq='15Min')
    #data['renewablespercentage'] = rengeneration["percentage"][:len(data)]

    # starting the data from the mentioned day
    data = data[data.index >= dateutil.parser.parse(start_date)]
    # only considering the target period of days, 24 hours per day and 4 observations per hour
    data = data.iloc[:int(target_period_days * 24 * 4), :]
    # calculating number of test data 24 hours per day and 4 observations per hour
    test_size = days_to_test * 24 * 4
    train_size = int(data.shape[0] - test_size)
    train, test = data.renewablespercentage[0:train_size], data.renewablespercentage[train_size:]
    # selecting defined columns for exog parameters
    exog_train, exog_test = data[0:train_size].iloc[:, [data.columns.get_loc(ex) for ex in exog_columns]], data[
                                                                                                           train_size:].iloc[
                                                                                                           :, [
                                                                                                               data.columns.get_loc(
                                                                                                                   ex)
                                                                                                               for ex in
                                                                                                               exog_columns]]
    # old way
    # exog_train,exog_test = data[0:train_size].loc[:, exog_columns], data[train_size:].loc[:, exog_columns]
    #print(test)
    return train, test, exog_train, exog_test, data


# create a set of sarima configs to try
def sarima_configs(t_params=['n', 'c', 't', 'ct']):
    models = list()
    # define config lists
    p_params = [i for i in range(4)]  # 12
    d_params = [i for i in range(3)]  # 4
    q_params = [i for i in range(4)]  # 12
    # t_params = ['ct']#['n','c','t','ct']
    P_params = [i for i in range(3)]  # 4
    D_params = [i for i in range(2)]  # 2
    Q_params = [i for i in range(3)]  # 4
    m_params = [4]  # weeks-months-seasons per year
    # create config instances
    for p in p_params:
        for d in d_params:
            for q in q_params:
                for t in t_params:
                    for P in P_params:
                        for D in D_params:
                            for Q in Q_params:
                                for m in m_params:
                                    cfg = [(p, d, q), (P, D, Q, m), t]
                                    # we use string to remove compare and remove the configs that have been executed already
                                    models.append(str(cfg))
    return models


def print_describe_residuals(resid, filename='qasd0078.txt'):
    with open(PATH + '{}'.format(filename), 'a+') as f:
        print("----------RESIDUALS----------", file=f)
        print(resid.describe(), file=f)
    f.close()


def save_photo(test, actual, outputname):
    fig = PLT.figure(num=None, figsize=(25, 10), dpi=80, facecolor='w', edgecolor='k')
    
    #######################
    '''   plt.plot(total_test_sarimax, label='Actual')
    plt.plot(fc_series_sarimax, label='Forecast-SARIMAX')
    plt.plot(fc_series_sarima, label='Forecast-SARIMA')
    plt.plot(fc_series_arimax, label='Forecast-ARIMAX')
    plt.title('Forecast vs Actuals',fontsize=font_size)
    plt.legend(loc='upper left')
    plt.xlabel('Time [YYYY-MM-DD]', fontsize=font_size)
    plt.ylabel('Percentage of the Renewables [%]', fontsize=font_size)
    plt.savefig(data_path + 'PLOT-TEST-ONLY{}.png'.format(output_file_name), format='png', dpi=1200)
    plt.show()
    plt.close(fig)
    '''
    ######################
    
    PLT.plot(test.index, test, test.index, actual, '-')

    #PLT.title('Forecast vs Actuals',fontsize=font_size)
    #PLT.legend(loc='upper left')
    #PLT.xlabel('Time [YYYY-MM-DD]', fontsize=font_size)
    #PLT.ylabel('Percentage of the Renewables [%]', fontsize=font_size)

    PLT.savefig(PATH + '{}.png'.format(outputname))
    # PLT.close()
    PLT.close(fig)


def print_model_into_file(modelFit, output):
    with open(PATH + '{}'.format(output), 'a+') as f:
        print(modelFit.summary(), file=f)
    f.close()


def mean_absolute_percentage_error(y_true, y_pred):
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100


def mean_percentage_error(y_true, y_pred):
    return np.mean((y_true - y_pred) / y_true) * 100


def mean_error(y_true, y_pred):
    return np.mean(y_true - y_pred)


def print_evaluation_metrics(test, prediction, label, outputname):
    with open(PATH + '{}'.format(outputname), 'a+') as f:
        print("-----------------{}-----------------".format(label), file=f)
        print("MSE", metrics.mean_squared_error(test, prediction), file=f)
        print("RMSE", sqrt(metrics.mean_squared_error(test, prediction)), file=f)
        print("MAE", metrics.mean_absolute_error(test, prediction), file=f)
        print("MAPE", mean_absolute_percentage_error(test, prediction), '%', file=f)
        print("MPE", mean_percentage_error(test, prediction), '%', file=f)
        print("mean Error", mean_error(test, prediction), file=f)
        print("R2S", metrics.r2_score(test, prediction), file=f)

    f.close()


def run_and_save_SARIMAX_model(train, test, exog_train, exog_test, config, output):
    order, sorder, trend = config
    model = SARIMAX(train, order=order, seasonal_order=sorder, trend=trend, exog=exog_train)
    model_fit = model.fit()
    residuals = model_fit.resid
    predict = model_fit.forecast(len(test), exog=exog_test)
    yhat = model_fit.predict()
    print_model_into_file(model_fit, '{}.txt'.format(output))
    save_photo(train, yhat, '{}-train--{}'.format(output, config))
    save_photo(test, predict, '{}-test-{}'.format(output, config))
    print_evaluation_metrics(train, yhat, "TRAIN", '{}.txt'.format(output))
    print_evaluation_metrics(test, predict, "TEST", '{}.txt'.format(output))
    print_describe_residuals(residuals, '{}.txt'.format(output))
    return model_fit, predict


def run_and_save_SARIMA_model(train, test, config, output):
    order, sorder, trend = config
    model = SARIMAX(train, order=order, seasonal_order=sorder, trend=trend)
    model_fit = model.fit()
    residuals = model_fit.resid
    predict = model_fit.forecast(len(test))
    yhat = model_fit.predict()
    print_model_into_file(model_fit, '{}.txt'.format(output))
    save_photo(train, yhat, '{}-train--{}'.format(output, config))
    save_photo(test, predict, '{}-test-{}'.format(output, config))
    print_evaluation_metrics(train, yhat, "TRAIN", '{}.txt'.format(output))
    print_evaluation_metrics(test, predict, "TEST", '{}.txt'.format(output))
    print_describe_residuals(residuals, '{}.txt'.format(output))
    return model_fit, predict


class summaryDto:
    def __init__(self, aic, summary):
        self.aic = aic
        self.summary = summary


def getAIC(summaryDTO):
    return summaryDTO.aic


def write_bests_to_file(output, summaries):
    with open(PATH + '{}.txt'.format(output), 'w+') as f:
        for m in summaries:
            print(m.summary, file=f)
            print(m.aic, file=f)
            # (m.summary)
    f.close()


def read_old_configs(old_configs_file_name):
    try:
        f = open(PATH + '{}.txt'.format(old_configs_file_name), 'r')
        lines = f.readlines()
        f.close()
        return lines
    except:
        f = open(PATH + '{}.txt'.format(old_configs_file_name), 'a+')
        f.close()
        return []


def updating_old_configs_file(old_configs_file_name, cfg):
    with open(PATH + '{}.txt'.format(old_configs_file_name), 'a+') as f:
        print(cfg, file=f)
    f.close()

def writing_model_execution_time(old_configs_file_name, time):
    with open(PATH+'{}-TIMES.txt'.format(old_configs_file_name), 'a+') as f:
        print(time, file=f)
    f.close()

def writing_total_execution_time(old_configs_file_name, time):
    with open(PATH+'{}TOTAL-times.txt'.format(old_configs_file_name), 'a+') as f:
        print(time, file=f)
    f.close()


def write_models_by_AIC(model_summaries, model_fit, size):
    try:
        model_summaries.sort(reverse=True, key=getAIC)
        if (len(model_summaries) > size):
            # model_summaries.sort(reverse=True, key=getAIC)
            model_summaries[0] = summaryDto(model_fit.aic, model_fit.summary())
        else:
            model_summaries.append(summaryDto(model_fit.aic, model_fit.summary()))
            # print(model_summaries)
        model_summaries.sort(reverse=True, key=getAIC)
        write_bests_to_file(output_file_name + "TOP_MODELS", model_summaries)
        return model_summaries
    except:
        print("Error in writing AIC")


def write_top_models_by_measure(model_summaries, model_summary_DTO, size, output_file_name):
    try:
        model_summaries.sort(reverse=True, key=getAIC)
        if (len(model_summaries) > size):
            # model_summaries.sort(reverse=True, key=getAIC)
            model_summaries[0] = model_summary_DTO
        else:
            model_summaries.append(model_summary_DTO)
            # print(model_summaries)
        model_summaries.sort(reverse=True, key=getAIC)
        write_bests_to_file(output_file_name + "TOP_MODELS", model_summaries)
        return model_summaries
    except:
        print("Error in writing " + label)



def walk_forward(output_file_name, config, old_configs_file_name, number_of_top_models, target_period_days,
                 days_to_test, start_date, dataset_name, exog_columns):
    with tf.device('/device:GPU:0'):
        total_test = pd.Series()
        total_predict = pd.Series()
        # creating the new folder path

        try:
            path = PATH + "walk-forward-results/" + str(config)
            print(path)
            os.mkdir(path)
        except OSError:
            print("ERROR Creation of the directory %s failed" % path)
        else:
            print("Successfully created the directory %s " % path)
        # splitting data and creating the model
        try:
            path = PATH + "walk-forward-results/" + str(config) + "/images"
            print(path)
            os.mkdir(path)
        except OSError:
            print("ERROR Creation of the directory %s failed" % path)
        else:
            print("Successfully created the directory %s " % path)
        try:
            for walking_parameter in range(1, days_to_test + 1):
                train, test, exog_train, exog_test, data = read_and_split_data(
                    target_period_days=target_period_days + walking_parameter, start_date=start_date, days_to_test=1,
                    exog_columns=exog_columns, file_name=dataset_name)
                # print(train.head) print(test.head)
                # print(train)
                
                model_fit, predict = run_and_save_SARIMAX_model(train, test, exog_train, exog_test, config,
                                                                "walk-forward-results/" + str(
                                                                    config) + '/' + output_file_name + str(
                                                                    config) + "DAY" + str(walking_parameter))
                print(config, walking_parameter)
                total_test = total_test.append(test)
                total_predict = total_predict.append(predict)
                # print("test\n",type(predict))
                # print("total predict\n",len(total_predict))
            # print("total test", total_test)
            # print("total predict", total_predict)
            fig = PLT.figure(num=None, figsize=(25, 10), dpi=80, facecolor='w', edgecolor='k')
            PLT.plot(total_test.index, total_test, total_test.index, total_predict, '-')
            PLT.savefig(PATH + "walk-forward-results/" + str(config) + '/images/{}.png'.format(output_file_name))
            PLT.close(fig)

            # intra day MPE, MAE interday
            error_avg = 0
            meanError = 0
            meanRMSE = 0
            meanMSE = 0
            meanMAE = 0
            meanMAPE = 0
            meanMPE = 0
            meanR2S = 0
            data_frequency_per_day = 96
            for day in range(days_to_test):
                error_avg += abs(
                    mean_percentage_error(total_test[day * data_frequency_per_day:(day + 1) * data_frequency_per_day],
                                          total_predict[
                                          day * data_frequency_per_day:(day + 1) * data_frequency_per_day]))
                meanError += abs(
                    mean_error(total_test[day * data_frequency_per_day:(day + 1) * data_frequency_per_day],
                               total_predict[
                               day * data_frequency_per_day:(day + 1) * data_frequency_per_day]))
                meanRMSE += abs(
                    sqrt(metrics.mean_squared_error(total_test[day * data_frequency_per_day:(day + 1) * data_frequency_per_day],
                                          total_predict[
                                          day * data_frequency_per_day:(day + 1) * data_frequency_per_day])))
                
                meanMSE += abs(
                    metrics.mean_squared_error(total_test[day * data_frequency_per_day:(day + 1) * data_frequency_per_day],
                                          total_predict[
                                          day * data_frequency_per_day:(day + 1) * data_frequency_per_day]))
                
                meanMAE += abs(
                    metrics.mean_absolute_error(total_test[day * data_frequency_per_day:(day + 1) * data_frequency_per_day],
                                          total_predict[
                                          day * data_frequency_per_day:(day + 1) * data_frequency_per_day]))
                
                meanMAPE += abs(
                    mean_absolute_percentage_error(total_test[day * data_frequency_per_day:(day + 1) * data_frequency_per_day],
                                          total_predict[
                                          day * data_frequency_per_day:(day + 1) * data_frequency_per_day]))
                
                meanMPE += abs(
                    mean_percentage_error(total_test[day * data_frequency_per_day:(day + 1) * data_frequency_per_day],
                                          total_predict[
                                          day * data_frequency_per_day:(day + 1) * data_frequency_per_day]))
                
                meanR2S += abs(
                    metrics.r2_score(total_test[day * data_frequency_per_day:(day + 1) * data_frequency_per_day],
                                          total_predict[
                                          day * data_frequency_per_day:(day + 1) * data_frequency_per_day]))
                
            with open(PATH + "walk-forward-results/" + str(config) + '/{}.txt'.format(output_file_name), 'a+') as f:
                print("-----------------{}-----------------".format(str(config)), file=f)
                print("MSE", metrics.mean_squared_error(total_test, total_predict), file=f)
                print("RMSE", sqrt(metrics.mean_squared_error(total_test, total_predict)), file=f)
                print("MAE", metrics.mean_absolute_error(total_test, total_predict), file=f)
                print("MAPE", mean_absolute_percentage_error(total_test, total_predict), '%', file=f)
                print("MPE", mean_percentage_error(total_test, total_predict), '%', file=f)
                print("R2S", metrics.r2_score(total_test, total_predict), file=f)
                print("Intra Day MPE, Inter day MAE", error_avg / days_to_test, file=f)
                print("MeanError", meanError / days_to_test, file=f)
                print("------------------", file=f)
                
                print("meanRMSE", meanRMSE / days_to_test, file=f)
                print("meanMSE", meanMSE / days_to_test, file=f)
                print("meanMAE", meanMAE / days_to_test, file=f)
                print("meanMAPE", meanMAPE / days_to_test, file=f)
                print("meanMPE", meanMPE / days_to_test, file=f)
                print("meanR2S", meanR2S / days_to_test, file=f)

            f.close()
            return model_fit, total_test, total_predict, train
        except:
            print("\n\n\n\nError in walk forward\n\n\n\n")
            print("Unexpected error:", sys.exc_info()[0])


def run_forward_all_models(output_file_name, config, old_configs_file_name, number_of_top_models, days_to_test,
                           target_period_days, start_date, dataset_name):
    model_summaries_AIC = []
    model_summaries_MPE = []
    model_summaries_RMSE = []
    model_summaries_MAPE = []
    model_summaries_MAE = []
    model_summaries_intra_MPE = []
    total_exectuion_time=timedelta()
    data_frequency_per_day = 96  # 24 hours * 4 freq/hour
    try:
            path = PATH + "walk-forward-results/" + "MODELSFORPLOT"
            print(path)
            os.mkdir(path)
    except OSError:
            print("ERROR Creation of the directory %s failed" % path)
    
    for cfg in config:
        try:
            start= datetime.now()
            model_fit, total_test, total_predict, train = walk_forward(output_file_name=output_file_name,
                                                                       config=cfg,
                                                                       old_configs_file_name=old_configs_file_name,
                                                                       number_of_top_models=number_of_top_models,
                                                                       days_to_test=days_to_test,
                                                                       target_period_days=target_period_days,
                                                                       start_date=start_date, dataset_name=dataset_name,
                                                                       exog_columns=exog_columns)
        except:
            print("following model was skiped:", str(cfg))

        try:
            model_summaries_MPE = write_top_models_by_measure(model_summaries_MPE, summaryDto(
                np.abs(mean_percentage_error(total_test, total_predict)), model_fit.summary()),
                                                              number_of_top_models,
                                                              "walk-forward-results/" + output_file_name + "--MPE-")
            model_summaries_RMSE = write_top_models_by_measure(model_summaries_RMSE, summaryDto(
                sqrt(metrics.mean_squared_error(total_test, total_predict)), model_fit.summary()),
                                                               number_of_top_models,
                                                               "walk-forward-results/" + output_file_name + "--RMSE-")
            model_summaries_MAPE = write_top_models_by_measure(model_summaries_MAPE, summaryDto(
                np.abs(mean_absolute_percentage_error(total_test, total_predict)), model_fit.summary()),
                                                               number_of_top_models,
                                                               "walk-forward-results/" + output_file_name + "--MAPE-")
            model_summaries_MAE = write_top_models_by_measure(model_summaries_MAE, summaryDto(
                np.abs(metrics.mean_absolute_error(total_test, total_predict)), model_fit.summary()),
                                                              number_of_top_models,
                                                              "walk-forward-results/" + output_file_name + "--MAE-")

            # intra day MPE, MAE interday
            error_avg = 0
            for day in range(days_to_test):
                error_avg += abs(
                    mean_percentage_error(
                        total_test[day * data_frequency_per_day:(day + 1) * data_frequency_per_day],
                        total_predict[
                        day * data_frequency_per_day:(day + 1) * data_frequency_per_day]))
            model_summaries_intra_MPE = write_top_models_by_measure(model_summaries_intra_MPE,
                                                                    summaryDto(error_avg / days_to_test,
                                                                               model_fit.summary()),
                                                                    number_of_top_models,
                                                                    "walk-forward-results/" + output_file_name + "--INTRA Day MPE- INTER Day MAe-")

            fc_series = pd.Series(total_predict, index=total_test.index)
            #lower_series = pd.Series(conf[:, 0], index=total_test.index)
            #upper_series = pd.Series(conf[:, 1], index=total_test.index)

            # Plot
            fig = plt.figure(figsize=(12, 5), dpi=100)
            plt.plot(train, label='training')
            plt.plot(total_test, label='actual')
            plt.plot(fc_series, label='forecast')
            #plt.fill_between(lower_series.index, lower_series, upper_series,
            #                 color='k', alpha=.15)
            plt.title('Forecast vs Actuals')
            plt.legend(loc='upper left', fontsize=8)
            plt.savefig(PATH + "walk-forward-results/" + str(cfg) + '/images/{}TRAINTEST.png'.format(output_file_name), format='png', dpi=1200)
            #plt.show()
            plt.close(fig)
            # updating old configs
            # for latter plot
            train.to_csv(PATH + "walk-forward-results/" + "MODELSFORPLOT/"+"{}-{}_train.csv".format(output_file_name,str(cfg)))
            total_test.to_csv(PATH + "walk-forward-results/" + "MODELSFORPLOT/"+"{}-{}_total_test.csv".format(output_file_name,str(cfg)))
            fc_series.to_csv(PATH + "walk-forward-results/" + "MODELSFORPLOT/"+"{}-{}_fc_series.csv".format(output_file_name,str(cfg)))


            end=datetime.now()
            # updating old configs
            updating_old_configs_file(old_configs_file_name, cfg)
            #writing execution time for the model
            writing_model_execution_time(old_configs_file_name, end-start)
            total_exectuion_time += end-start
            writing_total_execution_time(old_configs_file_name, total_exectuion_time)
        except:
            print("\n\n\n\nError in walking forward all models\n\n\n\n")
            print("Unexpected error:", sys.exc_info()[0])

        print(output_file_name)

def walk_forward_sarima(output_file_name, config, old_configs_file_name, number_of_top_models, target_period_days,
                 days_to_test, start_date, dataset_name, exog_columns):
    with tf.device('/device:GPU:0'):
        total_test = pd.Series()
        total_predict = pd.Series()
        # creating the new folder path

        try:
            path = PATH + "walk-forward-results/" + str(config)
            print(path)
            os.mkdir(path)
        except OSError:
            print("ERROR Creation of the directory %s failed" % path)
        else:
            print("Successfully created the directory %s " % path)
        # splitting data and creating the model
        try:
            path = PATH + "walk-forward-results/" + str(config) + "/images"
            print(path)
            os.mkdir(path)
        except OSError:
            print("ERROR Creation of the directory %s failed" % path)
        else:
            print("Successfully created the directory %s " % path)
        try:
            for walking_parameter in range(1, days_to_test + 1):
                train, test, exog_train, exog_test, data = read_and_split_data(
                    target_period_days=target_period_days + walking_parameter, start_date=start_date, days_to_test=1,
                    exog_columns=exog_columns, file_name=dataset_name)
                # print(train.head) print(test.head)
                # print(train)
                model_fit, predict = run_and_save_SARIMA_model(train, test, config,
                                                                "walk-forward-results/" + str(
                                                                    config) + '/' + output_file_name + str(
                                                                    config) + "DAY" + str(walking_parameter))
                print(config, walking_parameter)
                total_test = total_test.append(test)
                total_predict = total_predict.append(predict)
                # print("test\n",type(predict))
                # print("total predict\n",len(total_predict))
            # print("total test", total_test)
            # print("total predict", total_predict)
            fig = PLT.figure(num=None, figsize=(25, 10), dpi=80, facecolor='w', edgecolor='k')
            PLT.plot(total_test.index, total_test, total_test.index, total_predict, '-')
            PLT.savefig(PATH + "walk-forward-results/" + str(config) + '/images/{}.png'.format(output_file_name))
            PLT.close(fig)
            
            # intra day MPE, MAE interday
            error_avg = 0
            meanError = 0
            meanRMSE = 0
            meanMSE = 0
            meanMAE = 0
            meanMAPE = 0
            meanMPE = 0
            meanR2S = 0
            data_frequency_per_day = 96
            for day in range(days_to_test):
                error_avg += abs(
                    mean_percentage_error(total_test[day * data_frequency_per_day:(day + 1) * data_frequency_per_day],
                                          total_predict[
                                          day * data_frequency_per_day:(day + 1) * data_frequency_per_day]))
                meanError += abs(
                    mean_error(total_test[day * data_frequency_per_day:(day + 1) * data_frequency_per_day],
                               total_predict[
                               day * data_frequency_per_day:(day + 1) * data_frequency_per_day]))
                
                meanRMSE += abs(
                    sqrt(metrics.mean_squared_error(total_test[day * data_frequency_per_day:(day + 1) * data_frequency_per_day],
                                          total_predict[
                                          day * data_frequency_per_day:(day + 1) * data_frequency_per_day])))
                
                meanMSE += abs(
                    metrics.mean_squared_error(total_test[day * data_frequency_per_day:(day + 1) * data_frequency_per_day],
                                          total_predict[
                                          day * data_frequency_per_day:(day + 1) * data_frequency_per_day]))
                
                meanMAE += abs(
                    metrics.mean_absolute_error(total_test[day * data_frequency_per_day:(day + 1) * data_frequency_per_day],
                                          total_predict[
                                          day * data_frequency_per_day:(day + 1) * data_frequency_per_day]))
                
                meanMAPE += abs(
                    mean_absolute_percentage_error(total_test[day * data_frequency_per_day:(day + 1) * data_frequency_per_day],
                                          total_predict[
                                          day * data_frequency_per_day:(day + 1) * data_frequency_per_day]))
                
                meanMPE += abs(
                    mean_percentage_error(total_test[day * data_frequency_per_day:(day + 1) * data_frequency_per_day],
                                          total_predict[
                                          day * data_frequency_per_day:(day + 1) * data_frequency_per_day]))
                
                meanR2S += abs(
                    metrics.r2_score(total_test[day * data_frequency_per_day:(day + 1) * data_frequency_per_day],
                                          total_predict[
                                          day * data_frequency_per_day:(day + 1) * data_frequency_per_day]))
                
            with open(PATH + "walk-forward-results/" + str(config) + '/{}.txt'.format(output_file_name), 'a+') as f:
                print("-----------------{}-----------------".format(str(config)), file=f)
                print("MSE", metrics.mean_squared_error(total_test, total_predict), file=f)
                print("RMSE", sqrt(metrics.mean_squared_error(total_test, total_predict)), file=f)
                print("MAE", metrics.mean_absolute_error(total_test, total_predict), file=f)
                print("MAPE", mean_absolute_percentage_error(total_test, total_predict), '%', file=f)
                print("MPE", mean_percentage_error(total_test, total_predict), '%', file=f)
                print("R2S", metrics.r2_score(total_test, total_predict), file=f)
                print("Intra Day MPE, Inter day MAE", error_avg / days_to_test, file=f)
                print("MeanError", meanError / days_to_test, file=f)
                print("------------------", file=f)
                print("meanRMSE", meanRMSE / days_to_test, file=f)
                print("meanMSE", meanMSE / days_to_test, file=f)
                print("meanMAE", meanMAE / days_to_test, file=f)
                print("meanMAPE", meanMAPE / days_to_test, file=f)
                print("meanMPE", meanMPE / days_to_test, file=f)
                print("meanR2S", meanR2S / days_to_test, file=f)

            f.close()
            return model_fit, total_test, total_predict, train
        except:
            print("\n\n\n\nError in walk forward sarima\n\n\n\n")
            print("Unexpected error:", sys.exc_info()[0])


def run_forward_all_models_sarima(output_file_name, config, old_configs_file_name, number_of_top_models, days_to_test,
                           target_period_days, start_date, dataset_name):
    model_summaries_AIC = []
    model_summaries_MPE = []
    model_summaries_RMSE = []
    model_summaries_MAPE = []
    model_summaries_MAE = []
    model_summaries_intra_MPE = []
    data_frequency_per_day = 96  # 24 hours * 4 freq/hour
    total_exectuion_time=timedelta()

    for cfg in config:
        try:
            start= datetime.now()
            model_fit, total_test, total_predict, train = walk_forward_sarima(output_file_name=output_file_name,
                                                                       config=cfg,
                                                                       old_configs_file_name=old_configs_file_name,
                                                                       number_of_top_models=number_of_top_models,
                                                                       days_to_test=days_to_test,
                                                                       target_period_days=target_period_days,
                                                                       start_date=start_date, dataset_name=dataset_name,
                                                                       exog_columns=exog_columns)
        except:
            print("following model was skiped:", str(cfg))

        try:
            model_summaries_MPE = write_top_models_by_measure(model_summaries_MPE, summaryDto(
                np.abs(mean_percentage_error(total_test, total_predict)), model_fit.summary()),
                                                              number_of_top_models,
                                                              "walk-forward-results/" + output_file_name + "--MPE-")
            model_summaries_RMSE = write_top_models_by_measure(model_summaries_RMSE, summaryDto(
                sqrt(metrics.mean_squared_error(total_test, total_predict)), model_fit.summary()),
                                                               number_of_top_models,
                                                               "walk-forward-results/" + output_file_name + "--RMSE-")
            model_summaries_MAPE = write_top_models_by_measure(model_summaries_MAPE, summaryDto(
                np.abs(mean_absolute_percentage_error(total_test, total_predict)), model_fit.summary()),
                                                               number_of_top_models,
                                                               "walk-forward-results/" + output_file_name + "--MAPE-")
            model_summaries_MAE = write_top_models_by_measure(model_summaries_MAE, summaryDto(
                np.abs(metrics.mean_absolute_error(total_test, total_predict)), model_fit.summary()),
                                                              number_of_top_models,
                                                              "walk-forward-results/" + output_file_name + "--MAE-")

            # intra day MPE, MAE interday
            error_avg = 0
            for day in range(days_to_test):
                error_avg += abs(
                    mean_percentage_error(
                        total_test[day * data_frequency_per_day:(day + 1) * data_frequency_per_day],
                        total_predict[
                        day * data_frequency_per_day:(day + 1) * data_frequency_per_day]))
            model_summaries_intra_MPE = write_top_models_by_measure(model_summaries_intra_MPE,
                                                                    summaryDto(error_avg / days_to_test,
                                                                               model_fit.summary()),
                                                                    number_of_top_models,
                                                                    "walk-forward-results/" + output_file_name + "--INTRA Day MPE- INTER Day MAe-")

            orig = train

            orig.append(total_test)
            fc_series = pd.Series(total_predict, index=total_test.index)
            #lower_series = pd.Series(conf[:, 0], index=total_test.index)
            #upper_series = pd.Series(conf[:, 1], index=total_test.index)

            # Plot
            fig = plt.figure(figsize=(12, 5), dpi=100)
            plt.plot(train, label='training')
            plt.plot(total_test, label='actual')
            plt.plot(fc_series, label='forecast')
            #plt.fill_between(lower_series.index, lower_series, upper_series,
            #                 color='k', alpha=.15)
            plt.title('Forecast vs Actuals')
            plt.legend(loc='upper left', fontsize=8)
            plt.savefig(PATH + "walk-forward-results/" + str(cfg) + '/images/{}TRAINTEST.png'.format(output_file_name), format='png', dpi=1200)
            #plt.show()
            plt.close(fig)
            # updating old configs
            train.to_csv(PATH + "walk-forward-results/" + "MODELSFORPLOT/"+"{}-{}_train.csv".format(output_file_name,str(cfg)))
            total_test.to_csv(PATH + "walk-forward-results/" + "MODELSFORPLOT/"+"{}-{}_total_test.csv".format(output_file_name,str(cfg)))
            fc_series.to_csv(PATH + "walk-forward-results/" + "MODELSFORPLOT/"+"{}-{}_fc_series.csv".format(output_file_name,str(cfg)))

            end=datetime.now()
            # updating old configs
            updating_old_configs_file(old_configs_file_name, cfg)
            #writing execution time for the model
            writing_model_execution_time(old_configs_file_name, end-start)
            total_exectuion_time += end-start
            writing_total_execution_time(old_configs_file_name, total_exectuion_time)
        except:
            print("\n\n\n\nError in walking forward all models\n\n\n\n")
            print("Unexpected error:", sys.exc_info()[0])
        
        print(output_file_name)


### FANCY PLOTS FOR THE END ####
def plot_forecasts(data_path, sarima_name, sarimax_name, arimax_name, output_file_name):
    font_size = 14 # @param {type:"integer"}
    # train_sarima = pd.read_csv(data_path + "{}_train.csv".format(sarima_name), index_col=0)
    # train_sarima.index = pd.date_range(start=train_sarima.index[0], periods=len(train_sarima), freq='15Min')

    # total_test_sarima = pd.read_csv(data_path + "{}_total_test.csv".format(sarima_name), index_col=0)
    # total_test_sarima.index = pd.date_range(start=total_test_sarima.index[0], periods=len(total_test_sarima), freq='15Min')

    fc_series_sarima = pd.read_csv(data_path + "{}_fc_series.csv".format(sarima_name), index_col=0)
    fc_series_sarima.index = pd.date_range(start=fc_series_sarima.index[0], periods=len(fc_series_sarima), freq='15Min')

    train_sarimax = pd.read_csv(data_path + "{}_train.csv".format(sarimax_name), index_col=0)
    train_sarimax.index = pd.date_range(start=train_sarimax.index[0], periods=len(train_sarimax), freq='15Min')

    total_test_sarimax = pd.read_csv(data_path + "{}_total_test.csv".format(sarimax_name), index_col=0)
    total_test_sarimax.index = pd.date_range(start=total_test_sarimax.index[0], periods=len(total_test_sarimax),
                                             freq='15Min')

    fc_series_sarimax = pd.read_csv(data_path + "{}_fc_series.csv".format(sarimax_name), index_col=0)
    fc_series_sarimax.index = pd.date_range(start=fc_series_sarimax.index[0], periods=len(fc_series_sarimax),
                                            freq='15Min')

    # train_arimax = pd.read_csv(data_path + "{}_train.csv".format(arimax_name), index_col=0)
    # train_arimax.index = pd.date_range(start=train_arimax.index[0], periods=len(train_arimax), freq='15Min')

    # total_test_arimax = pd.read_csv(data_path + "{}_total_test.csv".format(arimax_name), index_col=0)
    # total_test_arimax.index = pd.date_range(start=total_test_arimax.index[0], periods=len(total_test_arimax), freq='15Min')

    fc_series_arimax = pd.read_csv(data_path + "{}_fc_series.csv".format(arimax_name), index_col=0)
    fc_series_arimax.index = pd.date_range(start=fc_series_arimax.index[0], periods=len(fc_series_arimax), freq='15Min')

    fig = plt.figure(figsize=(12, 5), dpi=100)
    # plt.plot(train_sarimax, label='Training')
    plt.plot(total_test_sarimax, label='Actual')
    plt.plot(fc_series_sarimax, label='Forecast-SARIMAX')
    plt.plot(fc_series_sarima, label='Forecast-SARIMA')
    plt.plot(fc_series_arimax, label='Forecast-ARIMAX')
    plt.title('Forecast vs Actuals',fontsize=font_size)
    plt.legend(loc='upper left')
    plt.xlabel('Time [YYYY-MM-DD]', fontsize=font_size)
    plt.ylabel('Percentage of the Renewables [%]', fontsize=font_size)
    plt.savefig(data_path + 'PLOT-TEST-ONLY{}.png'.format(output_file_name), format='png', dpi=1200)
    plt.show()
    plt.close(fig)

    fig = plt.figure(figsize=(12, 5), dpi=100)
    plt.plot(train_sarimax, label='Training')
    plt.plot(total_test_sarimax, label='Actual')
    plt.plot(fc_series_sarimax, label='Forecast-SARIMAX')
    plt.plot(fc_series_sarima, label='Forecast-SARIMA')
    plt.plot(fc_series_arimax, label='Forecast-ARIMAX')
    plt.title('Forecast vs Actuals', fontsize=font_size)
    plt.legend(loc='upper left')
    plt.xlabel('Time [YYYY-MM-DD]', fontsize=font_size)
    plt.ylabel('Percentage of the Renewables [%]', fontsize=font_size)
    plt.savefig(data_path + 'PLOT-All{}.png'.format(output_file_name), format='png', dpi=1200)
    plt.show()
    plt.close(fig)

"""##April Wind

SARIMAX
"""

################################  CONFIG  ############################################
# splitting train and test
exog_columns = ['windspeed','GHI']
target_period_days = 35  # @param {type:"integer"}
number_of_top_models = 20  # @param {type:"integer"}
start_test_date = "2019-04-01 00:00:00"  # @param {type:"string"}
days_to_test =   30# @param {type:"integer"}
dataset_name = 'Ultimate-12cities_spline_weather_entsoe_radiation_data' # @param ['weather_entsoe_radiation_data', 'Ultimate-12cities_spline_weather_entsoe_radiation_data','Ultimate-12cities_linear_weather_entsoe_radiation_data']
old_configs_file_name = "used-configs-{}Days".format(str(target_period_days))
t_param_string = "n"  # @param ['n', 'c', 't', 'ct']
output_file_name = "THE-BEST-SARIMAX-April-windOnly-DEDICATED"  # @param {type:"string"}

output_file_name = t_param_string + "-" + output_file_name
old_configs_file_name = output_file_name + old_configs_file_name
start_date= str(datetime.strptime(start_test_date,'%Y-%m-%d %H:%M:%S') - timedelta(days=target_period_days))
######################################################################################


cfg_list = [
#[(4, 1, 3), (2, 0, 2, 4), 'n'] #4
[(2, 1, 3), (2, 0, 2, 4), 'n'],
[(3, 1, 3), (2, 0, 2, 4), 'n'],
[(1, 1, 4), (2, 0, 2, 4), 'n'],
[(4, 1, 0), (2, 0, 2, 4), 'n'],
[(1, 1, 3), (2, 0, 2, 4), 'n'],
[(4, 1, 1), (2, 0, 2, 4), 'n'],
[(2, 1, 1), (2, 0, 2, 4), 'n'],
[(3, 1, 0), (2, 0, 2, 4), 'n'],
[(2, 1, 2), (2, 0, 2, 4), 'n'],
[(1, 1, 2), (2, 0, 2, 4), 'n']
]
try:
    path = PATH + "walk-forward-results"
    print(path)
    os.mkdir(path)
except OSError:
    print("ERROR Creation of the directory %s failed" % path)
else:
    print("Successfully created the directory %s " % path)

cfg_list = [str(cfg) for cfg in cfg_list]
old_configs = read_old_configs("walk-forward-results/" + old_configs_file_name)

# removing \n from each line
old_configs = [line.rstrip('\n') for line in old_configs]

# removing old configs from the current list
if (len(old_configs) > 0):
    cfg_list = list(set(cfg_list) ^ set(old_configs))

# converting from string to tuples
cfg_list = [ast.literal_eval(line) for line in cfg_list]
# cfg_list.sort(reverse=True)
# cfg_list.sort(reverse=True)
current_time = datetime.now().strftime("%d-%m-%Y %H:%M:%S")

print("Old configs size:",len(old_configs),"\nNew config size:", len(cfg_list))


run_forward_all_models(output_file_name=output_file_name + current_time, config=cfg_list,
                       old_configs_file_name="walk-forward-results/" + old_configs_file_name,
                       number_of_top_models=number_of_top_models, days_to_test=days_to_test,
                       target_period_days=target_period_days, start_date=start_date, dataset_name=dataset_name)

# cfg_list

"""SARIMA"""

################################  CONFIG  ############################################
# splitting train and test
exog_columns = ['windspeed', 'GHI']
target_period_days = 35  # @param {type:"integer"}
number_of_top_models = 20  # @param {type:"integer"}
start_test_date = "2019-04-01 00:00:00"  # @param {type:"string"}
days_to_test =   30# @param {type:"integer"}
dataset_name = 'Ultimate-12cities_spline_weather_entsoe_radiation_data' # @param ['weather_entsoe_radiation_data', 'Ultimate-12cities_spline_weather_entsoe_radiation_data','Ultimate-12cities_linear_weather_entsoe_radiation_data']
old_configs_file_name = "used-configs-{}Days".format(str(target_period_days))
t_param_string = "n"  # @param ['n', 'c', 't', 'ct']
output_file_name = "THE-BEST-SARIMA-April"  # @param {type:"string"}

output_file_name = t_param_string + "-" + output_file_name
old_configs_file_name = output_file_name + old_configs_file_name
start_date= str(datetime.strptime(start_test_date,'%Y-%m-%d %H:%M:%S') - timedelta(days=target_period_days))
######################################################################################


cfg_list = [
[(4, 1, 4), (2, 0, 2, 4), 'n'] #9

]
try:
    path = PATH + "walk-forward-results"
    print(path)
    os.mkdir(path)
except OSError:
    print("ERROR Creation of the directory %s failed" % path)
else:
    print("Successfully created the directory %s " % path)

cfg_list = [str(cfg) for cfg in cfg_list]
old_configs = read_old_configs("walk-forward-results/" + old_configs_file_name)

# removing \n from each line
old_configs = [line.rstrip('\n') for line in old_configs]

# removing old configs from the current list
if (len(old_configs) > 0):
    cfg_list = list(set(cfg_list) ^ set(old_configs))

# converting from string to tuples
cfg_list = [ast.literal_eval(line) for line in cfg_list]
# cfg_list.sort(reverse=True)
# cfg_list.sort(reverse=True)
current_time = datetime.now().strftime("%d-%m-%Y %H:%M:%S")

print("Old configs size:",len(old_configs),"\nNew config size:", len(cfg_list))


run_forward_all_models_sarima(output_file_name=output_file_name + current_time, config=cfg_list,
                       old_configs_file_name="walk-forward-results/" + old_configs_file_name,
                       number_of_top_models=number_of_top_models, days_to_test=days_to_test,
                       target_period_days=target_period_days, start_date=start_date, dataset_name=dataset_name)

# cfg_list

"""ARIMAX"""

################################  CONFIG  ############################################
# splitting train and test
exog_columns = ['windspeed','GHI']
target_period_days = 35  # @param {type:"integer"}
number_of_top_models = 20  # @param {type:"integer"}
start_test_date = "2019-04-01 00:00:00"  # @param {type:"string"}
days_to_test =   30# @param {type:"integer"}
dataset_name = 'Ultimate-12cities_spline_weather_entsoe_radiation_data' # @param ['weather_entsoe_radiation_data', 'Ultimate-12cities_spline_weather_entsoe_radiation_data','Ultimate-12cities_linear_weather_entsoe_radiation_data']
old_configs_file_name = "used-configs-{}Days".format(str(target_period_days))
t_param_string = "n"  # @param ['n', 'c', 't', 'ct']
output_file_name = "THE-BEST-ARIMAX-April-windOnlyP"  # @param {type:"string"}

output_file_name = t_param_string + "-" + output_file_name
old_configs_file_name = output_file_name + old_configs_file_name
start_date= str(datetime.strptime(start_test_date,'%Y-%m-%d %H:%M:%S') - timedelta(days=target_period_days))
######################################################################################


cfg_list = [
[(4, 1, 4), (0, 0, 0, 0), 'n'] #2
]
try:
    path = PATH + "walk-forward-results"
    print(path)
    os.mkdir(path)
except OSError:
    print("ERROR Creation of the directory %s failed" % path)
else:
    print("Successfully created the directory %s " % path)

cfg_list = [str(cfg) for cfg in cfg_list]
old_configs = read_old_configs("walk-forward-results/" + old_configs_file_name)

# removing \n from each line
old_configs = [line.rstrip('\n') for line in old_configs]

# removing old configs from the current list
if (len(old_configs) > 0):
    cfg_list = list(set(cfg_list) ^ set(old_configs))

# converting from string to tuples
cfg_list = [ast.literal_eval(line) for line in cfg_list]
# cfg_list.sort(reverse=True)
# cfg_list.sort(reverse=True)
current_time = datetime.now().strftime("%d-%m-%Y %H:%M:%S")

print("Old configs size:",len(old_configs),"\nNew config size:", len(cfg_list))


run_forward_all_models(output_file_name=output_file_name + current_time, config=cfg_list,
                       old_configs_file_name="walk-forward-results/" + old_configs_file_name,
                       number_of_top_models=number_of_top_models, days_to_test=days_to_test,
                       target_period_days=target_period_days, start_date=start_date, dataset_name=dataset_name)

# cfg_list

sarima_name="n-THE-BEST-ARIMAX-April-windOnly07-11-2020 13:07:35"# @param {type:"string"}
sarimax_name="n-THE-BEST-SARIMAX-April-windOnly07-11-2020 12:42:01"# @param {type:"string"}
arimax_name="n-THE-BEST-ARIMAX-April-windOnly07-11-2020 13:07:35"# @param {type:"string"}
output_file_name = "April-BEST-PLOT"# @param {type:"string"}


plot_forecasts(data_path=data_path, sarima_name=sarima_name, sarimax_name=sarimax_name, arimax_name=arimax_name, output_file_name=output_file_name)